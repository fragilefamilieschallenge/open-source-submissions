{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from MissingDataScript.py\n",
    "def fillMissing(inputcsv, outputcsv):\n",
    "    \n",
    "    # read input csv - takes time\n",
    "    df = pd.read_csv(inputcsv, low_memory=False);\n",
    "    # Fix date bug\n",
    "    df.cf4fint = ((pd.to_datetime(df.cf4fint) - pd.to_datetime('1960-01-01')) / np.timedelta64(1, 'D')).astype(int);\n",
    "    \n",
    "    # drop columns with all NA's (redacted columns)\n",
    "    df.dropna(axis=1,how='all',inplace=True);\n",
    "    \n",
    "    # replace remaining NA's with -10\n",
    "    df = df.fillna(-10, downcast='infer');\n",
    "    \n",
    "    # drop any column that contains a string\n",
    "    cols_to_remove = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            _ = df[col].astype(float)\n",
    "        except ValueError:\n",
    "            #print('Couldn\\'t covert %s to float' % col)\n",
    "            cols_to_remove.append(col)\n",
    "            pass\n",
    "\n",
    "    # keep only the columns in df that do not contain string\n",
    "    df = df[[col for col in df.columns if col not in cols_to_remove]];\n",
    "    \n",
    "    # if still NA, replace with 1\n",
    "    # df = df.fillna(value=1)\n",
    "    # replace negative values with 1\n",
    "    #num = df._get_numeric_data()\n",
    "    #num[num < 0] = 1\n",
    "    # write filled outputcsv\n",
    "    df.to_csv(outputcsv, index=False);\n",
    "    \n",
    "    return df;\n",
    "    \n",
    "# Usage:\n",
    "# fillMissing('background.csv', 'output.csv')\n",
    "# filleddf = pd.read_csv('output.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in data\n",
    "df_raw = fillMissing('fragilefamilieschallenge/background.csv','fragilefamilieschallenge/background_removeNA_and_strings.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('fragilefamilieschallenge/background_removeNA_and_strings.csv',low_memory=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sort data by challengeID\n",
    "df_sorted = df_raw.iloc[np.argsort(df_raw['challengeID'].values)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in training targets\n",
    "y_df = pd.read_csv('fragilefamilieschallenge/train.csv',low_memory=False);\n",
    "# get train ids\n",
    "train_ids = y_df['challengeID'].values.astype(int);\n",
    "# test ids are the ones not in train ids\n",
    "test_ids = np.setdiff1d(np.arange(1,4243),train_ids).astype(int);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before drop majority missing columns:  (4242, 10435)\n",
      "Number of values to impute:  29794879 ( 67.3098322906 %)\n",
      "Size after drop majority missing columns:  (4242, 1812)\n",
      "Number of values to impute:  1596008 ( 20.7637698491 %)\n",
      "Number of likely categorical variables:  1610 ( 89.3451720311 %)\n",
      "X_train.shape =  (2121, 4467)\n",
      "X_test.shape =  (2121, 4467)\n",
      "y_train.shape =  (2121, 6)\n",
      "label_names.shape =  (6,)\n",
      "feature_names.shape =  (4467,)\n",
      "train.shape =  (2121, 4473)\n",
      "Saving dataframe as imputed_1hot.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# remove column for idnum \n",
    "df = df_sorted.drop(['idnum'], axis=1);\n",
    "# index data by challengeID\n",
    "df.set_index('challengeID',inplace=True);\n",
    "\n",
    "# remove columns with zero variance\n",
    "varThresh = VarianceThreshold();\n",
    "varThresh.fit(df);\n",
    "df = df.iloc[:,varThresh.get_support()];\n",
    "\n",
    "# remove columns with other id#s\n",
    "df = df.drop(df.filter(regex='id\\d').axes[1].values,axis=1);\n",
    "\n",
    "# make new column with count of -9's\n",
    "df['missing9'] = df[df==-9.0].sum(axis=1)/-9.;\n",
    "\n",
    "print 'Size before drop majority missing columns: ', df.shape;\n",
    "nulls = df[df!=-10][df!=-9][df!=-8][df!=-7][df!=-6][df!=-5][df!=-4][df!=-3][df!=-2][df!=-1].isnull().sum().sum();\n",
    "print 'Number of values to impute: ', nulls, '(', 100.*nulls/(df.shape[0]*df.shape[1]),'%)';\n",
    "\n",
    "# drop any column that is more than half filled with missing values (not -1 or -2 though)\n",
    "# remaining missing values converted to NA\n",
    "#df = df[df>=-2].dropna(axis=1,thresh=2900,inplace=False);\n",
    "thresh = 2900\n",
    "#thresh = 3535;\n",
    "df = df[df!=-10][df!=-9][df!=-8][df!=-7][df!=-6][df!=-5][df!=-4][df!=-3][df!=-2][df!=-1].dropna(axis=1,thresh=thresh,inplace=False);\n",
    "\n",
    "print 'Size after drop majority missing columns: ', df.shape;\n",
    "\n",
    "nulls = df.isnull().sum().sum();\n",
    "print 'Number of values to impute: ', nulls, '(', 100.*nulls/(df.shape[0]*df.shape[1]),'%)'\n",
    "\n",
    "# impute remaining missing values (now NAs) with most frequent value in column\n",
    "df = df.apply(lambda x:x.fillna(x.value_counts().index[0],downcast='infer'));\n",
    "#df = df.fillna(df.mean(), downcast='infer');\n",
    "# drop any NAs that this may have caused\n",
    "df = df.dropna(axis=1, how='any',inplace=False);\n",
    "\n",
    "# do another variance filter\n",
    "varThresh.fit(df);\n",
    "df = df.iloc[:,varThresh.get_support()];\n",
    "\n",
    "# try to distinguish categorical and continuous variables\n",
    "likely_contin = pd.Series();\n",
    "for var in df.columns:\n",
    "    likely_contin[var] = df[var].between(20,99).any() or df[var].dtype==float or df[var].between(1940,2016).all();\n",
    "likely_cat = ~likely_contin;\n",
    "\n",
    "print 'Number of likely categorical variables: ', likely_cat.sum(), '(', 100.*likely_cat.sum()/df.shape[1], '%)' \n",
    "\n",
    "# one-hot encoding\n",
    "df = pd.get_dummies(df, prefix=likely_cat[likely_cat==True].axes[0].values, \n",
    "                    columns=likely_cat[likely_cat==True].axes[0].values);\n",
    "\n",
    "# determine variance threshold to get rid of categorical responses with frequency less than 100\n",
    "dum = np.zeros(4242);\n",
    "dum[:100]=1;\n",
    "thresh100 = dum.var();\n",
    "\n",
    "# a final variance threshold, for low variance 1-hot-encoded columns\n",
    "# effectively gets rid of low frequency categorical responses\n",
    "varThresh = VarianceThreshold(threshold=thresh100);\n",
    "varThresh.fit(df);\n",
    "df = df.iloc[:,varThresh.get_support()];\n",
    "\n",
    "# impute -2 and -1 for remaining (continuous) variables with column mean\n",
    "#df = df[df!=-2][df!=-1].fillna(df.mean());\n",
    "\n",
    "# separate data into training and testing sets\n",
    "X_train = df.iloc[train_ids-1];\n",
    "X_test = df.iloc[test_ids-1];\n",
    "# index labels by challengeID\n",
    "y_train = y_df.set_index('challengeID');\n",
    "\n",
    "# get column names \n",
    "feature_names = df.axes[1];\n",
    "label_names = y_train.axes[1];\n",
    "\n",
    "# combine training X and y into a single dataframe\n",
    "train = pd.concat([X_train,y_train],axis=1);\n",
    "\n",
    "print 'X_train.shape = ', X_train.shape\n",
    "print 'X_test.shape = ', X_test.shape\n",
    "print 'y_train.shape = ', y_train.shape\n",
    "print 'label_names.shape = ', label_names.shape\n",
    "print 'feature_names.shape = ', feature_names.shape\n",
    "print 'train.shape = ', train.shape\n",
    "\n",
    "df.to_csv('imputed_1hot.csv',index=False);\n",
    "print 'Saving dataframe as imputed_1hot.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict, ShuffleSplit\n",
    "from time import time\n",
    "def benchmark_all(train_all, X_all, est, label):\n",
    "    # remove rows that have no training outcomes for this label\n",
    "    train = train_all.dropna(axis=0, subset=[label]);\n",
    "    \n",
    "    # separate data and labels\n",
    "    X = train.iloc[:,:-6].values;\n",
    "    y = train[label].values;    \n",
    "\n",
    "    bootstrap = ShuffleSplit(n_splits=50, test_size=0.1, random_state=0)\n",
    "    \n",
    "    score_time = time();\n",
    "    mse = -cross_val_score(est, X, y, scoring='neg_mean_squared_error',cv=bootstrap, n_jobs=-1);\n",
    "    r2 = cross_val_score(est, X, y, scoring='r2',cv=bootstrap, n_jobs=-1);  \n",
    "    print '=============================================================';\n",
    "    print label;\n",
    "    print label, 'average mse score = ', mse.mean(), '+/- ', 2.*mse.std();\n",
    "    print label, 'average r2  score = ', r2.mean(), '+/- ', 2.*r2.std();\n",
    "    print label, 'bootstrapped scoring time = ', time()-score_time, 's';\n",
    "    fit_time = time();\n",
    "    est.fit(X,y);\n",
    "    predicted = est.predict(X_all);\n",
    "    print label, 'fit and predict time = ', time()-fit_time, 's';\n",
    "    \n",
    "    return mse, r2, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================\n",
      "gpa\n",
      "gpa average mse score =  0.379021759622 +/-  0.0821424549111\n",
      "gpa average r2  score =  0.126525476876 +/-  0.117641859916\n",
      "gpa bootstrapped scoring time =  80.4229331017 s\n",
      "gpa fit and predict time =  1.00913310051 s\n",
      "gpa alpha =  15.0\n",
      "Top 10 features:\n",
      "['m1h3_3' 'cm1adult_5' 'm2g13_4' 'cm1ethrace_4' 'm1i1_9' 'f1j7bc_101'\n",
      " 'm2c3i_2' 'f2h8a3_2' 'm2g1b_102' 'p5i28_3']\n",
      "Top continuous features are\n",
      "Feature # 763 : m2b5b\n",
      "Feature # 1430 : cm2povco\n",
      "Feature # 1518 : cm4povco\n",
      "=============================================================\n",
      "grit\n",
      "grit average mse score =  0.230280056004 +/-  0.0477060967391\n",
      "grit average r2  score =  0.0278524705266 +/-  0.0694803861616\n",
      "grit bootstrapped scoring time =  106.646890879 s\n",
      "grit fit and predict time =  1.43874692917 s\n",
      "grit alpha =  20.0\n",
      "Top 10 features:\n",
      "['m2c3a_1' 'm1i1_5' 'cf2hhimpb_6' 'cf2hhimp_6' 'cm5mint_1' 'p5q3de_1'\n",
      " 'm2c3i_3' 'cm4marp_1' 'o5f3_3' 'k5d1e_3']\n",
      "Top continuous features are\n",
      "Feature # 1506 : p5h15c\n",
      "Feature # 1617 : m4k22\n",
      "Feature # 1713 : m4b3\n",
      "=============================================================\n",
      "materialHardship\n",
      "materialHardship average mse score =  0.0201069300254 +/-  0.00623763916694\n",
      "materialHardship average r2  score =  0.166392217408 +/-  0.112251701733\n",
      "materialHardship bootstrapped scoring time =  106.919996977 s\n",
      "materialHardship fit and predict time =  1.85522389412 s\n",
      "materialHardship alpha =  15.0\n",
      "Top 10 features:\n",
      "['m5f23a_1' 'f1g9l_7' 'm3i7f_1' 'o5notinhouse_1' 'm4i23d_1' 'p5q1h_7'\n",
      " 'm2g3_12' 'm5f23c_1' 'k5e1d_0' 'p5i22e_3']\n",
      "Top continuous features are\n",
      "Feature # 1204 : p5h15c\n",
      "Feature # 1415 : p5h9\n",
      "Feature # 1708 : m4k22\n",
      "=============================================================\n",
      "eviction\n",
      "eviction average mse score =  0.0548472010569 +/-  0.0288249526571\n",
      "eviction average r2  score =  0.0404208881573 +/-  0.0628307035581\n",
      "eviction bootstrapped scoring time =  118.984602928 s\n",
      "eviction fit and predict time =  1.69016098976 s\n",
      "eviction alpha =  15.0\n",
      "Top 10 features:\n",
      "['o5notinhouse_1' 'm4k12_108' 'm5f23c_1' 'm5i3c_1' 'm5f23b_1' 'm5f23k_1'\n",
      " 'm2k10bc_106' 'o5g5_1' 'p5q2b_3' 'm3b4f_1']\n",
      "Top continuous features are\n",
      "Feature # 1037 : m2b15\n",
      "Feature # 1551 : p5h15c\n",
      "Feature # 1693 : p5h8\n",
      "=============================================================\n",
      "layoff\n",
      "layoff average mse score =  0.169723437852 +/-  0.0391646197655\n",
      "layoff average r2  score =  -0.00893899870692 +/-  0.0501512288105\n",
      "layoff bootstrapped scoring time =  74.7861289978 s\n",
      "layoff fit and predict time =  0.99777007103 s\n",
      "layoff alpha =  20.0\n",
      "Top 10 features:\n",
      "['p5j2e_3' 'f1c8_2' 'm1b15a_4' 'cm3span_1' 'm1d3b_4' 'cf1adult_5'\n",
      " 'm4b4a4_0' 'cm2biok_5' 'cm4tele_1' 'm5f7a_1']\n",
      "Top continuous features are\n",
      "Feature # 1213 : p5h15c\n",
      "Feature # 1418 : m2b5b\n",
      "Feature # 1541 : pcg5stat\n",
      "=============================================================\n",
      "jobTraining\n",
      "jobTraining average mse score =  0.172403787066 +/-  0.0329134047475\n",
      "jobTraining average r2  score =  0.0403345254903 +/-  0.0573882853213\n",
      "jobTraining bootstrapped scoring time =  109.36975193 s\n",
      "jobTraining fit and predict time =  1.83382296562 s\n",
      "jobTraining alpha =  15.0\n",
      "Top 10 features:\n",
      "['p5l13f_1' 'm2g1a_105' 'k5e2d_3' 'cm4tele_0' 'm5i1_1' 'm4k3b_1' 'm3b4l_1'\n",
      " 'm1b15f_5' 'o5d1_2_2' 'f1g9e_3']\n",
      "Top continuous features are\n",
      "Feature # 777 : m3k22\n",
      "Feature # 934 : m2b5b\n",
      "Feature # 1386 : cm5povco\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.linear_model import RidgeCV\n",
    "norm = Normalizer(copy=False);\n",
    "estimator = RidgeCV(alphas=(10.0, 15.0, 20.0), cv=None, fit_intercept=True, normalize=True, scoring='neg_mean_squared_error');\n",
    "predicted_all = pd.read_csv('fragilefamilieschallenge/prediction.csv');\n",
    "X_all = df.values;\n",
    "for label in label_names:\n",
    "    mse, r2, predicted = benchmark_all(train, X_all, estimator, label);\n",
    "    print label, 'alpha = ', estimator.alpha_;\n",
    "    estimator_coefs = estimator.coef_;\n",
    "    feature_importance = np.argsort(estimator_coefs)[::-1];\n",
    "    print 'Top 10 features:';\n",
    "    print feature_names[feature_importance].values[:10];\n",
    "    feature_values = pd.DataFrame(data=np.sort(estimator_coefs)[::-1],index=feature_names[feature_importance].values,columns=['Weight']);\n",
    "    feats = feature_values.axes[0].values;\n",
    "    cont_feats = [s for s in feats if \"_\" not in s];\n",
    "    top_cont_feat_idx = np.argwhere(feats==cont_feats[0])\n",
    "    top_cont_feat = feats[top_cont_feat_idx];\n",
    "    print 'Top continuous features are'\n",
    "    for i in range(3):\n",
    "        top_cont_feat_idx = np.argwhere(feats==cont_feats[i])\n",
    "        top_cont_feat = feats[top_cont_feat_idx];\n",
    "        print 'Feature #', top_cont_feat_idx[0][0], ':', top_cont_feat[0][0]\n",
    "    feature_values.to_csv('results/ridge_feature_rank_lowvar_'+label+'.csv', index=True);\n",
    "    predicted_all[label] = predicted;\n",
    "\n",
    "predicted_all.to_csv('results/ridge_impute_1hot_prediction_all.csv',index=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
